{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "6FR9hytRrGVp"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "headers = {\n",
        "    'Authorization': 'Bearer <OPEN_AI_KEY>',\n",
        "    'Content-Type': 'application/json',\n",
        "}\n",
        "MODEL_TYPE = \"gpt-3.5-turbo-0613\" # gpt-4\n",
        "examples = json.load(open(\"/PCA-EVAL/pca-eval/data/v1.0/Autonomous Driving/meta_data.json\",\"r\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "esxpRad42Z7g"
      },
      "outputs": [],
      "source": [
        "def format_choices(choices):\n",
        "    # example: ['Phoenix', 'Baton Rouge', 'Honolulu', 'Cheyenne'] -> \"(A) Phoenix. (B) Baton Rouge. (C) Honolulu. (D) Cheyenne.\"\n",
        "    return \" \".join([f\"({chr(ord('A') + i)}) {choice}\" for i, choice in enumerate(choices)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "g-xQxDI4sH81"
      },
      "outputs": [],
      "source": [
        "start_prompt = \"\"\"\n",
        "You are a professional multimodal embodied reasoner. Your are assisted with multiple visual api which can answer your questions about an  image. Your job is to select the best action to answer my question based on an  image.  Note that you can't directly see the image but through the answer of API. I will first give you the description of valid APIs and then give you the question. You can gather information from the api when giving the answer.\n",
        "\"\"\"\n",
        "\n",
        "api_prompt = \"# API Description:\\ndef detect_traffic_sign():\\n    \\\"\\\"\\\"\\n    Detects traffic signs in the image.\\n    :return: list of detected traffic signs and coordinates, e.g. ['stop','max speed limit']\\n    \\\"\\\"\\\"\\n    pass\\n\\ndef object_detection():\\n    \\\"\\\"\\\"\\n    Detects objects in the image.\\n    :return: dict of detected objects and number of the objects, e.g. {'car':10, 'person':1}\\n    \\\"\\\"\\\"\\n    pass\\n\\ndef ocr():\\n    \\\"\\\"\\\"\\n    Performs OCR on the image.\\n    :return: list of detected text, e.g. ['Coffee Shop', 'Open 24/7']\\n    \\\"\\\"\\\"\\n    pass\\n\\ndef image_caption():\\n    \\\"\\\"\\\"\\n    Generates a caption for the image.\\n    :return: caption, e.g. 'A red car driving down the street'\\n    \\\"\\\"\\\"\\n    pass\\n\\ndef weather_detection():\\n    \\\"\\\"\\\"\\n    Detect current weather.\\n    :return: weather, e.g. 'rainy' or 'clear'\\n    \\\"\\\"\\\"\\n    pass\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSTxp7-rmejY",
        "outputId": "d4efae00-9278-411f-d908-1404975de710"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# API Description:\n",
            "def detect_traffic_sign():\n",
            "    \"\"\"\n",
            "    Detects traffic signs in the image.\n",
            "    :return: list of detected traffic signs and coordinates, e.g. ['stop','max speed limit']\n",
            "    \"\"\"\n",
            "    pass\n",
            "\n",
            "def object_detection():\n",
            "    \"\"\"\n",
            "    Detects objects in the image.\n",
            "    :return: dict of detected objects and number of the objects, e.g. {'car':10, 'person':1}\n",
            "    \"\"\"\n",
            "    pass\n",
            "\n",
            "def ocr():\n",
            "    \"\"\"\n",
            "    Performs OCR on the image.\n",
            "    :return: list of detected text, e.g. ['Coffee Shop', 'Open 24/7']\n",
            "    \"\"\"\n",
            "    pass\n",
            "\n",
            "def image_caption():\n",
            "    \"\"\"\n",
            "    Generates a caption for the image.\n",
            "    :return: caption, e.g. 'A red car driving down the street'\n",
            "    \"\"\"\n",
            "    pass\n",
            "\n",
            "def weather_detection():\n",
            "    \"\"\"\n",
            "    Detect current weather.\n",
            "    :return: weather, e.g. 'rainy' or 'clear'\n",
            "    \"\"\"\n",
            "    pass\n"
          ]
        }
      ],
      "source": [
        "print(api_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2JmnML3vmgS"
      },
      "source": [
        "# 新段落"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "1V4wls_c0Tjh"
      },
      "outputs": [],
      "source": [
        "def chat_traffic(example_json,model=\"gpt-4\"):\n",
        "    goal_prompt = example_json['question']\n",
        "    actions_str = format_choices(example_json['actions'])\n",
        "    answer = example_json['answer_index']\n",
        "\n",
        "\n",
        "    signs = str(example_json['api_cached_result']['detect_traffic_sign'])\n",
        "    caption = example_json['api_cached_result']['caption']\n",
        "    objects = str(example_json['api_cached_result']['object_detection'])\n",
        "    weather = example_json['api_cached_result']['weather_detection']\n",
        "    ocr = example_json['api_cached_result']['ocr']\n",
        "\n",
        "    request = {\n",
        "    \"model\": model,\n",
        "    \"messages\": [\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": start_prompt\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"assistant\",\n",
        "      \"content\": \"Sure, please provide me with the description of the valid APIs and your question.\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": api_prompt\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": \"Thank you for providing the descriptions of the valid APIs. Please go ahead and ask your question so that I can assist you in selecting the best action based on the image.\"\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"Question: {} Actions: {}\".format(goal_prompt,actions_str)\n",
        "    },\n",
        "    ]\n",
        "    }\n",
        "\n",
        "    api_call_history = {\n",
        "        \"detect_traffic_sign\":0,\n",
        "        \"object_detection\":0,\n",
        "        \"caption\":0,\n",
        "        \"weather_detection\":0,\n",
        "        \"ocr\":0\n",
        "    }\n",
        "\n",
        "    while True:\n",
        "      while True:\n",
        "        try:\n",
        "          response = requests.post('https://api.openai.com/v1/chat/completions',\n",
        "                         headers=headers,\n",
        "                         data=json.dumps(request))\n",
        "          model_response_json = json.loads(response.text)['choices'][0]['message']\n",
        "\n",
        "          break\n",
        "        except Exception as e:\n",
        "          continue\n",
        "\n",
        "\n",
        "      # check whether api call exists in the last response\n",
        "\n",
        "\n",
        "      print(model_response_json)\n",
        "\n",
        "      request['messages'].append(model_response_json)\n",
        "\n",
        "      api_response = \"\"\n",
        "      has_api_call = 0\n",
        "\n",
        "      if \"detect_traffic_sign\" in model_response_json['content'] and not api_call_history['detect_traffic_sign']:\n",
        "        api_response += \"detect_traffic_sign() = \"+signs+\"\\n\"\n",
        "        has_api_call = 1\n",
        "        api_call_history['detect_traffic_sign'] = 1\n",
        "\n",
        "\n",
        "      if \"object_detection\" in model_response_json['content'] and not api_call_history['object_detection']:\n",
        "        api_response += \"object_detection() = \"+objects+\"\\n\"\n",
        "        has_api_call = 1\n",
        "        api_call_history['object_detection'] = 1\n",
        "\n",
        "      if \"caption\" in model_response_json['content'] and not api_call_history['caption']:\n",
        "        api_response += \"caption() = \"+caption+\"\\n\"\n",
        "        has_api_call = 1\n",
        "        api_call_history['caption'] = 1\n",
        "\n",
        "      if \"weather_detection\" in model_response_json['content'] and not api_call_history['weather_detection']:\n",
        "        api_response += \"weather_detection() = \"+weather+\"\\n\"\n",
        "        has_api_call = 1\n",
        "        api_call_history['weather_detection'] = 1\n",
        "\n",
        "      if \"ocr\" in model_response_json['content'] and not api_call_history['ocr']:\n",
        "        api_response += \"ocr() = \"+ocr+\"\\n\"\n",
        "        has_api_call = 1\n",
        "        api_call_history['ocr'] = 1\n",
        "\n",
        "\n",
        "\n",
        "      request['messages'].append({\n",
        "          \"role\":\"user\",\n",
        "          \"content\":api_response\n",
        "      })\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      if not has_api_call:\n",
        "        break\n",
        "\n",
        "\n",
        "\n",
        "    return request"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-xoFrzoc3Du5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7X9SF-Av_cJ2",
        "outputId": "a655d75f-7bb4-4423-f97d-bff57f6de9bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'version': 1.0,\n",
              " 'domain': 'Autonomous Driving',\n",
              " 'index': 0,\n",
              " 'image': 'traffic_000.jpg',\n",
              " 'question': 'You are a driving assistant. The car is at the speed of 50 km/h. Based on current image, what is the best action to take?',\n",
              " 'actions': ['Slow down',\n",
              "  'keep driving',\n",
              "  'Stop the car',\n",
              "  'Change to other lane',\n",
              "  'Speed up'],\n",
              " 'answer_index': 5,\n",
              " 'reason': 'Based on the image, the lowest speed for current lane is 70km/h, current speed is 50km/h, which is lower than the limit, so you need to speed up.',\n",
              " 'key_concept': ['Minimum Speed Limit 70 km/h', 'clear road'],\n",
              " 'api_cached_result': {'detect_traffic_sign': ['Maximum Speed Limit 100 km/h',\n",
              "   'Minimum Speed Limit 70 km/h'],\n",
              "  'object_detection': [['car', 0], ['people', 0], ['bicycle', 0]],\n",
              "  'caption': 'a car driving down a highway with signs on it',\n",
              "  'weather_detection': 'clear',\n",
              "  'ocr': 'None'}}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "examples[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zkbNRykT3ByA"
      },
      "outputs": [],
      "source": [
        "model_answers = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONkcjv6J57pD"
      },
      "outputs": [],
      "source": [
        "for i in tqdm(examples):\n",
        "  model_answers.append(chat_traffic(i,MODEL_TYPE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMA-74PfUA-x"
      },
      "outputs": [],
      "source": [
        "with open(\"traffic_chatgpt_3.5_answer_full_dialog.json\",\"w\") as f:\n",
        "  json.dump(model_answers,f,indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "GFuEywIkSlUR"
      },
      "outputs": [],
      "source": [
        "assert len(model_answers) == len(examples)\n",
        "\n",
        "# write answers\n",
        "outputs = []\n",
        "\n",
        "for i,j in enumerate(model_answers):\n",
        "  outputs.append({\"index\":i,\"model_output\":j['messages'][-2]['content']})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Y_cIqRIW5-3H"
      },
      "outputs": [],
      "source": [
        "with open(\"traffic_chatgpt_3.5_answer.json\",\"w\") as f: # used for automatic evaluation\n",
        "  json.dump(outputs,f,indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "FxxAI3YOTMrl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
